{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T15:52:42.965521Z",
     "start_time": "2018-08-01T15:52:42.197228Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T15:52:42.973745Z",
     "start_time": "2018-08-01T15:52:42.967662Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_time_stepped_data(df, time_steps,num_features):\n",
    "    d = []\n",
    "    for i in range(time_steps):\n",
    "        d.append(df.shift(-i).values[:-time_steps].reshape(-1,num_features+1))\n",
    "    return np.transpose(np.array(d),(1,0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T15:52:43.137806Z",
     "start_time": "2018-08-01T15:52:42.975600Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_formatted_data(df, time_steps, num_features, fut_type=1):\n",
    "    assert (time_steps%2 !=0), \"Time steps should be odd!\"\n",
    "    d = create_time_stepped_data(df, time_steps, num_features)\n",
    "    \n",
    "    past = d[:,:int(time_steps/2),:]\n",
    "    if(fut_type == 1):\n",
    "        fut = np.flip(d[:,int(time_steps/2)+1:,:],1)\n",
    "    else:\n",
    "        fut = np.flip(d[:,:int(time_steps/2),:],1)\n",
    "    y = d[:,int(time_steps/2),-1]\n",
    "    if(num_features == 0):\n",
    "        cur = np.zeros((d.shape[0],1,256))\n",
    "    else:\n",
    "        cur = d[:,int(time_steps/2),:-1]\n",
    "        cur = np.dstack((cur.reshape(-1,1,num_features),np.zeros(shape=(d.shape[0],1,256-num_features))))\n",
    "#     for i in range(past.shape[0]):\n",
    "#         for j in range(past.shape[1]):\n",
    "#             past[i][j] = (past[i][j] - df.mean())/df.var()\n",
    "#             fut[i][j] = (fut[i][j] - df.mean())/df.var()\n",
    "    return past,fut,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T15:52:43.247287Z",
     "start_time": "2018-08-01T15:52:43.141063Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_split(past,fut,y,df):\n",
    "    train_split = int(0.8*y.shape[0])\n",
    "    \n",
    "    #'df' is a pd series.\n",
    "#     mx = df.iloc[:train_split].max()\n",
    "#     mn = df.iloc[:train_split].min()\n",
    "    \n",
    "#     return (past[:train_split]-mn)/(mx-mn), (past[train_split:]-mn)/(mx-mn), (fut[:train_split]-mn)/(mx-mn), np.zeros_like(past[train_split:]), y[:train_split], y[train_split:]\n",
    "    return past[:train_split], past[train_split:], fut[:train_split], np.zeros_like(past[train_split:]), y[:train_split], y[train_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T15:52:43.580362Z",
     "start_time": "2018-08-01T15:52:43.250577Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, past_shape, fut_shape, hidden_dim):\n",
    "        super(Net,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.conv1_p = nn.Conv1d(past_shape[1],32,5)\n",
    "        self.conv2_p = nn.Conv1d(32,32,3)\n",
    "        self.lin1_p = nn.Linear(32,16)\n",
    "        \n",
    "        self.conv1_f = nn.Conv1d(fut_shape[1],32,5)\n",
    "        self.conv2_f = nn.Conv1d(32,32,3)\n",
    "        self.lin1_f = nn.Linear(32,16)\n",
    "                \n",
    "        self.lstm1 = nn.LSTM(16,self.hidden_dim,dropout=0.2)\n",
    "        self.lin1 = nn.Linear(self.hidden_dim,16)\n",
    "        self.lstm2 = nn.LSTM(16,self.hidden_dim,dropout=0.2)\n",
    "        self.lin2 = nn.Linear(self.hidden_dim,16)\n",
    "        self.lstm3 = nn.LSTM(16,self.hidden_dim,dropout=0.2)\n",
    "        self.lin3 = nn.Linear(self.hidden_dim,16)\n",
    "        \n",
    "        self.fc1 = nn.Linear(256,64)\n",
    "        self.fc2 = nn.Linear(64,64)\n",
    "        self.fc3 = nn.Linear(64,64)\n",
    "        self.fc4 = nn.Linear(64,64)\n",
    "        self.fc5 = nn.Linear(64,64)\n",
    "        self.fc6 = nn.Linear(64,64)\n",
    "        self.fc7 = nn.Linear(64,1)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (torch.autograd.Variable(torch.zeros(1, 8, self.hidden_dim).cuda()),\n",
    "                torch.autograd.Variable(torch.zeros(1, 8, self.hidden_dim).cuda()))\n",
    "\n",
    "    def forward(self, past, fut):\n",
    "        conv_p = F.relu6(self.conv1_p(past))\n",
    "        conv_p = F.relu6(self.conv2_p(conv_p))\n",
    "        conv_p = conv_p.view(-1,conv_p.size()[2],conv_p.size()[1])\n",
    "        conv_p = F.relu6(self.lin1_p(conv_p))\n",
    "        \n",
    "        conv_f = F.relu6(self.conv1_f(fut))\n",
    "        conv_f = F.relu6(self.conv2_f(conv_f))\n",
    "        conv_f = conv_f.view(-1,conv_f.size()[2],conv_f.size()[1])\n",
    "        conv_f = F.relu6(self.lin1_f(conv_f))\n",
    "        \n",
    "        lstm_inp = torch.cat([conv_p,conv_f],dim=1)        \n",
    "        \n",
    "        lstm_out1, self.hidden = self.lstm1(lstm_inp,self.hidden)\n",
    "        lstm_out1 = F.relu6(self.lin1(lstm_out1))\n",
    "        lstm_out2, self.hidden = self.lstm1(lstm_out1,self.hidden)\n",
    "        lstm_out2 = F.relu6(self.lin2(lstm_out2))\n",
    "        lstm_out3, self.hidden = self.lstm1(lstm_out2,self.hidden)\n",
    "        lstm_out3 = F.relu6(self.lin3(lstm_out3))\n",
    "        \n",
    "        dnn_inp = torch.cat([lstm_inp,lstm_out3],dim=1)\n",
    "        dnn_inp = dnn_inp.view(-1,dnn_inp.size()[1]*dnn_inp.size()[2])\n",
    "        \n",
    "        out = F.relu6(self.fc1(dnn_inp))\n",
    "        out = F.relu6(self.fc2(out))\n",
    "        out = F.relu6(self.fc3(out))\n",
    "        out = F.relu6(self.fc4(out))\n",
    "        out = F.relu6(self.fc5(out))\n",
    "        out = F.relu6(self.fc6(out))\n",
    "        out = F.relu6(self.fc7(out))\n",
    "                \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T15:52:44.462020Z",
     "start_time": "2018-08-01T15:52:43.582581Z"
    }
   },
   "outputs": [],
   "source": [
    "stocks = ['AMZN','FB','WMT']\n",
    "data = pd.read_csv(\"all_stocks_5yr.csv\")\n",
    "data = data[data['Name'] == stocks[2]][['close']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T15:52:44.474595Z",
     "start_time": "2018-08-01T15:52:44.464425Z"
    }
   },
   "outputs": [],
   "source": [
    "time_steps = 21\n",
    "num_features = 0\n",
    "past,fut,y = create_formatted_data(data, time_steps, num_features)\n",
    "past_train, past_test, fut_train, fut_test, y_train, y_test = train_test_split(past,fut,y,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T15:52:44.591544Z",
     "start_time": "2018-08-01T15:52:44.476568Z"
    }
   },
   "outputs": [],
   "source": [
    "past_train = np.transpose(past_train,(0,2,1))\n",
    "fut_train = np.transpose(fut_train,(0,2,1))\n",
    "\n",
    "past_test = np.transpose(past_test,(0,2,1))\n",
    "fut_test = np.transpose(fut_test,(0,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T15:52:44.704677Z",
     "start_time": "2018-08-01T15:52:44.594957Z"
    }
   },
   "outputs": [],
   "source": [
    "past_train1 = torch.autograd.Variable(torch.from_numpy(past_train.copy())).float()\n",
    "fut_train1 = torch.autograd.Variable(torch.from_numpy(fut_train.copy())).float()\n",
    "y_train1 = torch.autograd.Variable(torch.from_numpy(y_train.copy())).float()\n",
    "\n",
    "past_test1 = torch.autograd.Variable(torch.from_numpy(past_test.copy())).float()\n",
    "fut_test1 = torch.autograd.Variable(torch.from_numpy(fut_test.copy())).float()\n",
    "y_test1 = torch.autograd.Variable(torch.from_numpy(y_test.copy())).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T13:26:03.046902Z",
     "start_time": "2018-08-02T13:26:02.929233Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Net(past_train1.size(),fut_train1.size(),32)\n",
    "model.cuda()\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.(model.parameters(), lr=0.1)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T13:26:03.606208Z",
     "start_time": "2018-08-02T13:26:03.552526Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Net(\n",
       "  (conv1_p): Conv1d (1, 32, kernel_size=(5,), stride=(1,))\n",
       "  (conv2_p): Conv1d (32, 32, kernel_size=(3,), stride=(1,))\n",
       "  (lin1_p): Linear(in_features=32, out_features=16)\n",
       "  (conv1_f): Conv1d (1, 32, kernel_size=(5,), stride=(1,))\n",
       "  (conv2_f): Conv1d (32, 32, kernel_size=(3,), stride=(1,))\n",
       "  (lin1_f): Linear(in_features=32, out_features=16)\n",
       "  (lstm1): LSTM(16, 32, dropout=0.2)\n",
       "  (lin1): Linear(in_features=32, out_features=16)\n",
       "  (lstm2): LSTM(16, 32, dropout=0.2)\n",
       "  (lin2): Linear(in_features=32, out_features=16)\n",
       "  (lstm3): LSTM(16, 32, dropout=0.2)\n",
       "  (lin3): Linear(in_features=32, out_features=16)\n",
       "  (fc1): Linear(in_features=256, out_features=64)\n",
       "  (fc2): Linear(in_features=64, out_features=64)\n",
       "  (fc3): Linear(in_features=64, out_features=64)\n",
       "  (fc4): Linear(in_features=64, out_features=64)\n",
       "  (fc5): Linear(in_features=64, out_features=64)\n",
       "  (fc6): Linear(in_features=64, out_features=64)\n",
       "  (fc7): Linear(in_features=64, out_features=1)\n",
       ")>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-02T13:27:28.190121Z",
     "start_time": "2018-08-02T13:26:04.239591Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------\n",
      "batch loss [1     1] loss: 2.310\n",
      "batch loss [1    33] loss: 2.441\n",
      "batch loss [1    65] loss: 2.345\n",
      "batch loss [1    97] loss: 2.382\n",
      "batch loss [1   129] loss: 2.291\n",
      "batch loss [1   161] loss: 2.393\n",
      "batch loss [1   193] loss: 2.433\n",
      "batch loss [1   225] loss: 2.296\n",
      "batch loss [1   257] loss: 2.331\n",
      "batch loss [1   289] loss: 2.376\n",
      "batch loss [1   321] loss: 2.319\n",
      "batch loss [1   353] loss: 2.274\n",
      "batch loss [1   385] loss: 2.305\n",
      "batch loss [1   417] loss: 2.396\n",
      "batch loss [1   449] loss: 2.551\n",
      "batch loss [1   481] loss: 2.489\n",
      "batch loss [1   513] loss: 2.347\n",
      "batch loss [1   545] loss: 2.219\n",
      "batch loss [1   577] loss: 2.085\n",
      "batch loss [1   609] loss: 1.971\n",
      "batch loss [1   641] loss: 1.799\n",
      "batch loss [1   673] loss: 1.649\n",
      "batch loss [1   705] loss: 1.737\n",
      "batch loss [1   737] loss: 1.886\n",
      "batch loss [1   769] loss: 1.953\n",
      "batch loss [1   801] loss: 1.964\n",
      "batch loss [1   833] loss: 2.087\n",
      "batch loss [1   865] loss: 2.082\n",
      "batch loss [1   897] loss: 2.010\n",
      "batch loss [1   929] loss: 2.012\n",
      "batch loss [1   961] loss: 1.964\n",
      "Epochs 1 loss: 2.18816\n",
      "-----------------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------------\n",
      "batch loss [2     1] loss: 2.123\n",
      "batch loss [2    33] loss: 2.258\n",
      "batch loss [2    65] loss: 2.167\n",
      "batch loss [2    97] loss: 2.208\n",
      "batch loss [2   129] loss: 2.122\n",
      "batch loss [2   161] loss: 2.230\n",
      "batch loss [2   193] loss: 2.276\n",
      "batch loss [2   225] loss: 2.145\n",
      "batch loss [2   257] loss: 2.187\n",
      "batch loss [2   289] loss: 2.240\n",
      "batch loss [2   321] loss: 2.192\n",
      "batch loss [2   353] loss: 2.159\n",
      "batch loss [2   385] loss: 2.206\n",
      "batch loss [2   417] loss: 2.317\n",
      "batch loss [2   449] loss: 2.501\n",
      "batch loss [2   481] loss: 2.488\n",
      "batch loss [2   513] loss: 2.347\n",
      "batch loss [2   545] loss: 2.219\n",
      "batch loss [2   577] loss: 2.085\n",
      "batch loss [2   609] loss: 1.971\n",
      "batch loss [2   641] loss: 1.799\n",
      "batch loss [2   673] loss: 1.649\n",
      "batch loss [2   705] loss: 1.737\n",
      "batch loss [2   737] loss: 1.886\n",
      "batch loss [2   769] loss: 1.953\n",
      "batch loss [2   801] loss: 1.964\n",
      "batch loss [2   833] loss: 2.087\n",
      "batch loss [2   865] loss: 2.082\n",
      "batch loss [2   897] loss: 2.010\n",
      "batch loss [2   929] loss: 2.012\n",
      "batch loss [2   961] loss: 1.964\n",
      "Epochs 2 loss: 2.15402\n",
      "-----------------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------------\n",
      "batch loss [3     1] loss: 2.123\n",
      "batch loss [3    33] loss: 2.258\n",
      "batch loss [3    65] loss: 2.167\n",
      "batch loss [3    97] loss: 2.208\n",
      "batch loss [3   129] loss: 2.122\n",
      "batch loss [3   161] loss: 2.230\n",
      "batch loss [3   193] loss: 2.276\n",
      "batch loss [3   225] loss: 2.145\n",
      "batch loss [3   257] loss: 2.187\n",
      "batch loss [3   289] loss: 2.240\n",
      "batch loss [3   321] loss: 2.192\n",
      "batch loss [3   353] loss: 2.159\n",
      "batch loss [3   385] loss: 2.206\n",
      "batch loss [3   417] loss: 2.317\n",
      "batch loss [3   449] loss: 2.501\n",
      "batch loss [3   481] loss: 2.488\n",
      "batch loss [3   513] loss: 2.347\n",
      "batch loss [3   545] loss: 2.219\n",
      "batch loss [3   577] loss: 2.085\n",
      "batch loss [3   609] loss: 1.971\n",
      "batch loss [3   641] loss: 1.799\n",
      "batch loss [3   673] loss: 1.649\n",
      "batch loss [3   705] loss: 1.737\n",
      "batch loss [3   737] loss: 1.886\n",
      "batch loss [3   769] loss: 1.953\n",
      "batch loss [3   801] loss: 1.964\n",
      "batch loss [3   833] loss: 2.087\n",
      "batch loss [3   865] loss: 2.082\n",
      "batch loss [3   897] loss: 2.010\n",
      "batch loss [3   929] loss: 2.012\n",
      "batch loss [3   961] loss: 1.964\n",
      "Epochs 3 loss: 2.14265\n",
      "-----------------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------------\n",
      "batch loss [4     1] loss: 2.123\n",
      "batch loss [4    33] loss: 2.258\n",
      "batch loss [4    65] loss: 2.167\n",
      "batch loss [4    97] loss: 2.208\n",
      "batch loss [4   129] loss: 2.122\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-acb243e25551>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "overall_loss = 0.0\n",
    "model.hidden = model.init_hidden()\n",
    "\n",
    "for e in range(10):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    print(\"-----------------------------------------------------------------------------------------------\")    \n",
    "    for i in range(0,past_train1.size(0),batch_size):\n",
    "        # get the inputs\n",
    "        past, fut, labels = past_train1[i:i+batch_size].cuda(), fut_train1[i:i+batch_size].cuda(), y_train1[i:i+batch_size].cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(past,fut)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        overall_loss += loss.data\n",
    "        batch_loss = loss.data\n",
    "        # print statistics\n",
    "        print('batch loss [%d %5d] loss: %.3f' % (e+1, i+1, batch_loss / batch_size))\n",
    "        \n",
    "#         plt.figure(figsize=(10,10))\n",
    "#         plt.plot(outputs.data.cpu().numpy(),label='pred')\n",
    "#         plt.plot(labels.data.cpu().numpy(),label='true')\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "        \n",
    "    print('Epochs %d loss: %.5f' % (e + 1, overall_loss / ((e+1)*past_train1.size(0))))\n",
    "    print(\"-----------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
