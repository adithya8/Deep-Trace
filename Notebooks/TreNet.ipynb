{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T09:30:26.713556Z",
     "start_time": "2018-05-19T09:30:06.159288Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, date\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Conv1D, Conv2D, Flatten, MaxPool1D, MaxPool2D, AvgPool1D, AvgPool2D, Masking, RepeatVector, Add, Maximum, Average, BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU, ELU\n",
    "from keras.models import Model, Sequential\n",
    "from livelossplot import PlotLossesKeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T09:42:48.798822Z",
     "start_time": "2018-05-19T09:42:48.785403Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"PopcornSmallProphetInputDF.csv\")\n",
    "changepoints = pickle.load(open(\"changepoints.pkl\",'rb'), encoding='latin').index\n",
    "changepoints = changepoints.insert(0,0)\n",
    "changepoints = sorted(changepoints.insert(-1,df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T09:42:49.249509Z",
     "start_time": "2018-05-19T09:42:49.126672Z"
    }
   },
   "outputs": [],
   "source": [
    "col_idx = 1\n",
    "m = -1\n",
    "\n",
    "for i in range(len(changepoints)-1):\n",
    "    if((changepoints[i+1]-changepoints[i]) > m): m = (changepoints[i+1]-changepoints[i])\n",
    "\n",
    "d = pd.DataFrame()\n",
    "for i in range(len(changepoints)-1):\n",
    "    seq = df.values[changepoints[i]:changepoints[i+1],col_idx]\n",
    "    lk = np.array([seq.shape[0]])\n",
    "    sk,_,_,_,_ = stats.linregress(x=list(range(lk[0])), y=seq.astype('float'))\n",
    "    tempZeros = np.zeros((m+2-tempTuple.shape[0],))\n",
    "    zpTuple = np.hstack((tempZeros,seq))\n",
    "    tempTuple = (np.hstack((np.hstack((lk,np.array(sk))),zpTuple)))\n",
    "    tempTuple = tempTuple.reshape(1,-1)\n",
    "    d = pd.concat([d,pd.DataFrame(tempTuple)], axis=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T06:45:09.679450Z",
     "start_time": "2018-05-19T06:45:09.466322Z"
    }
   },
   "source": [
    "df = pd.read_csv(\"notNiteeshData.csv\")\n",
    "trunc_df = df[df.columns[list(range(0,7))+list(range(81,95))]]\n",
    "trunc_df['tot_occ'] = trunc_df['english']+trunc_df['hindi']+trunc_df['tamil']+trunc_df['others']\n",
    "trunc_df = pd.concat([trunc_df[trunc_df.columns[:5]],trunc_df['tot_occ'],trunc_df[trunc_df.columns[5:-1]]], axis=1)\n",
    "trunc_df.drop(list(range(360,405)),inplace=True, axis=0)\n",
    "trunc_df = trunc_df[trunc_df[\"ES_POPCORN SMALL\"]>75]\n",
    "trunc_df = trunc_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T10:38:38.422088Z",
     "start_time": "2018-05-19T10:38:38.324648Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_3d_data(data, timesteps, n_features):\n",
    "    for i in range(timesteps):\n",
    "        data = pd.concat([data,data.iloc[:,-n_features:].shift(-1)], axis=1)\n",
    "    x=data.iloc[:-timesteps,:-n_features]\n",
    "    y=data.iloc[:-timesteps,-n_features+2:]\n",
    "    return x,y\n",
    "\n",
    "def data_creation(df,l_col_idx, timesteps, n_features):\n",
    "    data = df[df.columns[l_col_idx]]\n",
    "    x,y = convert_3d_data(data, timesteps, n_features)\n",
    "    print(x.shape, y.shape)\n",
    "    \n",
    "    x_cnn = pd.DataFrame()\n",
    "    x_lstm = pd.DataFrame()\n",
    "    for i in range(timesteps):\n",
    "        x_cnn = pd.concat([x_cnn,pd.DataFrame(x.values[:,i*(n_features)+2:i*(n_features)+n_features])], axis=1)\n",
    "        x_lstm = pd.concat([x_lstm,pd.DataFrame(x.values[:,i*(n_features):i*(n_features)+2])], axis=1)\n",
    "    print(x_cnn.shape, x_lstm.shape)\n",
    "    train_split = int(0.7*x_cnn.shape[0])\n",
    "    x_cnn_train = x_cnn[:train_split].values\n",
    "    x_cnn_test = x_cnn[train_split:].values\n",
    "    x_lstm_train = x_lstm[:train_split].values\n",
    "    x_lstm_test = x_lstm[train_split:].values\n",
    "    y_train = y[:train_split].values\n",
    "    y_test = y[train_split:].values\n",
    "        \n",
    "    return x_cnn_train, x_lstm_train, y_train, x_cnn_test, x_lstm_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T10:38:40.172141Z",
     "start_time": "2018-05-19T10:38:40.137102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 379) (24, 188)\n",
      "(24, 375) (24, 4)\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "5       0\n",
      "6       0\n",
      "7       0\n",
      "8       0\n",
      "9       0\n",
      "10      0\n",
      "11      0\n",
      "12      0\n",
      "13      0\n",
      "14      0\n",
      "15      0\n",
      "16      0\n",
      "17      0\n",
      "18      0\n",
      "19      0\n",
      "20      0\n",
      "21      0\n",
      "22      0\n",
      "23      0\n",
      "24      0\n",
      "25      0\n",
      "26      0\n",
      "27      0\n",
      "28      0\n",
      "29      0\n",
      "       ..\n",
      "157    24\n",
      "158    24\n",
      "159    24\n",
      "160    24\n",
      "161    24\n",
      "162    24\n",
      "163    24\n",
      "164    24\n",
      "165    24\n",
      "166    24\n",
      "167    24\n",
      "168    24\n",
      "169    24\n",
      "170    24\n",
      "171    24\n",
      "172    24\n",
      "173    24\n",
      "174    24\n",
      "175    24\n",
      "176    24\n",
      "177    24\n",
      "178    24\n",
      "179    24\n",
      "180    24\n",
      "181    24\n",
      "182    24\n",
      "183    24\n",
      "184    24\n",
      "185    24\n",
      "186    24\n",
      "Length: 375, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "l = d.columns\n",
    "x_cnn_train, x_lstm_train, y_train, x_cnn_test, x_lstm_test, y_test = data_creation(d,l,2,m+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T08:57:20.318173Z",
     "start_time": "2018-05-19T08:57:20.219042Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_run(time_steps, cnn_shape, x_cnn_train, x_lstm_train, y_train, x_cnn_test, x_lstm_test, y_test):\n",
    "    \n",
    "    LSTM_inp = Input(shape=(time_steps,2))    \n",
    "    x1 = LSTM(100)(LSTM_inp)\n",
    "    x1 = LeakyReLU(0.3)(x1)\n",
    "    x1 = Dropout(0.15)(x1)\n",
    "    x1 = Dense(300)(x1)\n",
    "    x1 = LeakyReLU(0.3)(x1)\n",
    "    out_lstm = Dropout(0.2)(x1)\n",
    "    \n",
    "    CNN_inp = Input(shape=cnn_shape)\n",
    "    x = Conv1D(filters=32,kernel_size=4)(CNN_inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(filters=32,kernel_size=4)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool1D(pool_size=2)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(300)(x)\n",
    "    out_cnn = LeakyReLU(0.3)(x)\n",
    "    \n",
    "    feat_fus = Add()([out_lstm,out_cnn])\n",
    "    feat_fus = LeakyReLU(0.25)(feat_fus)\n",
    "    feat_fus = Dense(cnn_shape[0])(feat_fus)\n",
    "    feat_fus = LeakyReLU(0.3)(feat_fus)\n",
    "    out_model = Dropout(0.12)(feat_fus)\n",
    "    \n",
    "    model = Model(inputs=[LSTM_inp,CNN_inp], outputs=[out_model], name='TreRNN')\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae','acc'])\n",
    "    model.summary()\n",
    "    model.fit([x_lstm,x_cnn], y_train, callbacks=[PlotLossesKeras()], verbose=0, batch_size=50, epochs=100, validation_data=([x_lstm_test,x_cnn_test],y_test))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T09:34:27.674968Z",
     "start_time": "2018-05-19T09:34:27.669094Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 190)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_run(10,(1,1), )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
